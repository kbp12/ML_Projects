# -*- coding: utf-8 -*-
"""
Created on Mon Feb  1 18:00:27 2021
Created by Kedar Badrinath Pathade

Objective - Using Multiple Regression Backward Elimination Method to determine a variable 
            which can be approximated as the only independent variable and creating a 
            linear model which can be treated as a simple linear regressor model for
            predicting the result.

The dataset used is saved at this link
https://drive.google.com/file/d/1rgZDni5IIHJZczaAOobeT6pHmiB45jw8/view?usp=sharing

@author: DELL
"""
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#importing required libraries for execution
import pandas as pd

#now creating a dataframe from the given dataset 
dataset = pd.read_csv("50_Startups.csv")
print(dataset)
print(type(dataset))

#Notice that we have categorical variables in our dataset 
#hence converting those variables in numerical form 
pd.get_dummies(dataset["State"])
pd.get_dummies(dataset["State"], drop_first=True)
variable = pd.get_dummies(dataset["State"], drop_first=True)

#now we will concatenate dataset and variable 
dataset = pd.concat([dataset, variable], axis= 1)

#now we don't need the State column in our dataset so we will remove it 
dataset.drop(["State"], axis = 1 , inplace = True)
print(dataset.head(5))

#now creating independent and dependent variables from the dataset
x = dataset.iloc[ : , [0,1,2,4,5]].values
y = dataset.iloc[ : , [3]].values

#Now we will import the funcion which splits the data into training and testing parts
from sklearn.model_selection import train_test_split
xtrain , xtest , ytrain , ytest = train_test_split(x , y , test_size = 1/5 , random_state = 1)

#Fitting the training dataset to Linear Regression model
from sklearn.linear_model import LinearRegression
function = LinearRegression()
function.fit(x , y)

print(function.coef_)
print(function.intercept_)

#Now predicting the test dataset
ypred = function.predict(xtest)
print(ypred)

#Now calculating the accuracy of our model
from sklearn.metrics import r2_score
r2_score( ytest , ypred)


--------------------------Backward Elimination------------------------------------------------


#Backward Elimination technique is the feature selection technique. It is used
#to remove those features that do not have significant effect on dependent variable
#or prediction of output.

#Preparation of Backward elimination

#importing the library
import statsmodels.api as sm

#adding a column in matrix of features
import numpy as nm
x = nm.append(arr = nm.ones((50,1)).astype(int), values = x , axis = 1 )

#Applying backward elimination process now
#Firstly we will create a new feature vector x_opt, which will only contain a set of 
#independent features that are significantly affecting the dependent variable.
x_opt=x[:, [ 0,1,2,3,4,5]]

#for fitting the model, we will create a regressor_OLS object of new class OLS of 
#statsmodels library. Then we will fit it by using the fit() method.
regressor_OLS=sm.OLS(endog = y, exog=x_opt).fit()

#We will use summary() method to get the summary table of all the variables.
regressor_OLS.summary()

#In the above summary table, we can clearly see the p-values of all the variables. 
#Here x1 is R&D spend, x2 is Adminstration spend, x3 is Marketing spend and x4 & x5 are dummy variales. 

#Now since x5 has highest p-value greater than 0.05, hence, will remove the x5 variable
#(dummy variable) from the table and will refit the model.
x_opt= x[:, [0,1,2,3,4]]
regressor_OLS=sm.OLS(endog = y, exog=x_opt).fit()
regressor_OLS.summary()

#Now since x4 has highest p-value greater than 0.05, hence, will remove the x4 variable
#(dummy variable) from the table and will refit the model.
x_opt= x[:, [0,1,2,3]]
regressor_OLS=sm.OLS(endog = y, exog=x_opt).fit()
regressor_OLS.summary()

#Now we will remove the Admin spend (x2) which is having .602 p-value and
# again refit the model.
x_opt= x[:, [0,1,3]]
regressor_OLS=sm.OLS(endog = y, exog=x_opt).fit()
regressor_OLS.summary()

#Finally, we will remove one more variable, which has .60 p-value for marketing spend,
#that is more than significant level value of 0.05
x_opt= x[:, [0,1]]
regressor_OLS=sm.OLS(endog = y, exog=x_opt).fit()
regressor_OLS.summary()

#Hence,only  R&D independent variable is a significant variable for the prediction. 
#So we can now predict efficiently using this variable.


-----------------Building Linear Regression model by only using R&D spend:------------------------------


#again importing the dataset
newdataset = pd.read_csv("50_Startups.csv")
print(newdataset)

#extracting the independent variable R&D spend and dependent variable profit from newdataset
xnew = newdataset.iloc[ : , [0]].values
ynew = newdataset.iloc[ : , [4]].values
print(xnew)
print(ynew)

#splitting the dataset into trnining and testing 
from sklearn.model_selection import train_test_split
xtrainnew , xtestnew , ytrainnew, ytestnew = train_test_split(xnew, ynew, test_size = 1/5, random_state = 1)

#Fitting the train dataset into model
from sklearn.linear_model import LinearRegression
function_new = LinearRegression()
function_new.fit(xtrainnew, ytrainnew)

#printing the values of coefficient and intercept
print(function_new.coef_)
print(function_new.intercept_)

#Predicting the test set values
yprednew = function_new.predict(xtestnew)

#Calculating the accuracy of model using r2_square value
from sklearn.metrics import r2_score
r2_score( ytestnew, yprednew)
#The above score tells that our model is now more accurate with the test dataset with
#accuracy equal to 96.1%

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
